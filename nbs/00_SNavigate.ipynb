{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# snavigate\n",
    "\n",
    "> Module containing the creation, functionalities and closing of webdriver for Sales Navigator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp snavigate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from selenium import webdriver    # to  control the chrome browser\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import ElementNotVisibleException, ElementNotSelectableException\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from browsermobproxy import Server\n",
    "import pandas as pd\n",
    "from retry import retry\n",
    "from requests.exceptions import RequestException\n",
    "\n",
    "from SNavigator.parser import *\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from bs4 import BeautifulSoup     # to parse the page source\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "load_dotenv('/Volumes/Users/matu/pass.env')\n",
    "\n",
    "path_to_browsermobproxy = \"/Volumes/Users/matu/Documents/Xcode/browsermob-proxy-2.1.4/bin/\"\n",
    "url = 'https://www.linkedin.com/sales/login'\n",
    "username = os.environ.get('SN_USER')\n",
    "passw = os.environ.get('SN_PASS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Navigator():\n",
    "    def __init__(self\n",
    "                 ,headless=False, #Make the Chromium Headless or not\n",
    "                 login = True #If should login inmediatly\n",
    "                ) -> None:\n",
    "        super().__init__( )\n",
    "        self.server = Server(path_to_browsermobproxy\n",
    "            + \"browsermob-proxy\", options={'port': 8090})\n",
    "        self.server.start()\n",
    "        self.proxy = self.server.create_proxy(params={\"trustAllServers\": \"true\"})\n",
    "        chrome_options = Options()\n",
    "        chrome_options.add_argument(\"--ignore-certificate-errors\")\n",
    "        chrome_options.headless=headless\n",
    "        chrome_options.add_argument(f\"--proxy-server={self.proxy.proxy}\")\n",
    "        self.driver = webdriver.Chrome('/Volumes/Users/matu/Documents/Xcode/chromedriver',options=chrome_options )\n",
    "        if login:\n",
    "            self.login()\n",
    "\n",
    "    def login(self)->None:\n",
    "        \"\"\"\n",
    "        Method to loging to Sales NAvigator from the webrDriver created\n",
    "        \"\"\"\n",
    "        self.driver.get(url)\n",
    "        time.sleep(random.randint(5,10))\n",
    "        frame = self.driver.find_element(By.TAG_NAME, 'iframe')\n",
    "        self.driver.switch_to.frame(frame)       # entering the username and password\n",
    "        user = self.driver.find_element(By.ID, 'username')\n",
    "        user.send_keys(username)\n",
    "        password = self.driver.find_element(By.ID, 'password')\n",
    "        password.send_keys(passw)\n",
    "        singin = self.driver.find_element(By.TAG_NAME, 'button')\n",
    "        singin.click()\n",
    "        wait = WebDriverWait(self.driver, timeout=30, poll_frequency=2, ignored_exceptions=[ElementNotVisibleException, ElementNotSelectableException])\n",
    "        wait.until(lambda d: d.find_element(By.CLASS_NAME,\"homepage__right-column\"))\n",
    "        # time.sleep(random.randint(10,20))\n",
    "        \n",
    "\n",
    "    def close(self)->None:\n",
    "        \"\"\"\n",
    "        Close the Webdriver\n",
    "        \"\"\"\n",
    "        self.server.stop()\n",
    "        self.driver.close()\n",
    "\n",
    "    \n",
    "    def scroll_bottom(self,\n",
    "                      t_time: int=50, #Total time to spend in hte webpage\n",
    "                      pause: int=1,#Time pause to scroll one more step\n",
    "                      # total_steps:int = 10, #Number of steps to go down\n",
    "                      move = True\n",
    "                     )->None:\n",
    "        \"\"\"\n",
    "        Method to scroll on the website\n",
    "        \"\"\"\n",
    "        if not move:\n",
    "            time.sleep(t_time)\n",
    "            return\n",
    "        total_steps = round(t_time/pause)\n",
    "        # Get scroll height\n",
    "        initial_height = 0\n",
    "        last_height = self.driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        stepsize = last_height/total_steps \n",
    "        while True:\n",
    "            # Wait to load page\n",
    "            time.sleep(pause)\n",
    "            # Calculate new scroll height and compare with last scroll height\n",
    "            self.driver.execute_script(f\"window.scrollTo({0}, {initial_height});\")\n",
    "            initial_height += stepsize  # Scroll down to bottom\n",
    "            if stepsize == 0:\n",
    "                break\n",
    "            elif initial_height >= last_height:\n",
    "                break\n",
    "\n",
    "    \n",
    "    def next_page(self, url=None)->bool:\n",
    "        \"\"\"\n",
    "        Method to select button for next page, if exist\n",
    "        \"\"\"\n",
    "        _ = self.proxy.new_har(f\"list.har\", options={'captureHeaders': False,'captureContent': True, 'captureBinaryContent': True})\n",
    "        time.sleep(5)\n",
    "        # First webpage will have url, so a HAR will be started and then url will be loaded\n",
    "        if url:\n",
    "            self.driver.get(url)\n",
    "            time.sleep(5)\n",
    "            next_b = self.driver.find_element(By.CLASS_NAME, 'artdeco-pagination__button--next')\n",
    "            self.driver.execute_script(f\"window.scrollTo({0}, {next_b.location_once_scrolled_into_view['y']});\")    \n",
    "            time.sleep(2)\n",
    "            return True\n",
    "        else:\n",
    "            next_b = self.driver.find_element(By.CLASS_NAME, 'artdeco-pagination__button--next')\n",
    "            self.driver.execute_script(f\"window.scrollTo({0}, {next_b.location_once_scrolled_into_view['y']});\")    \n",
    "            time.sleep(2)\n",
    "            \n",
    "        if next_b.is_enabled():\n",
    "            next_b.click()\n",
    "            time.sleep(7)\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def get_companies_ids(self)->list:\n",
    "        \"\"\"\n",
    "        Method to get companies Ids from a website\n",
    "        \"\"\"\n",
    "        soup = BeautifulSoup(self.driver.page_source, 'html.parser')\n",
    "        companies = soup.find_all('a', attrs={'class':'lists-detail__view-company-name-link'})\n",
    "        ids = [f\"{company.get('href').split('/')[-1]}\" for company in companies]\n",
    "        return ids\n",
    "\n",
    "    def scrap_ids(self,\n",
    "                  url=None\n",
    "                 )->list:\n",
    "        \"\"\"\n",
    "        Methods to visit a url and get all companies ids from there\n",
    "        \"\"\"\n",
    "        if url:\n",
    "            self.driver.get(url)\n",
    "        time.sleep(random.randint(5,15))            \n",
    "        self.scroll_bottom(15)\n",
    "        ids = self.get_companies_ids()\n",
    "        if self.next_page():\n",
    "            ids.extend(self.scrap_ids())\n",
    "        return ids\n",
    "    \n",
    "    def get_url(self,\n",
    "                url:str #Url string to visit and return the address\n",
    "               )->str:\n",
    "        \"\"\"\n",
    "        Method that return the url it is loading\n",
    "        \"\"\"\n",
    "        self.driver.get(url)\n",
    "\n",
    "    def parse_search(self,\n",
    "                     url:str =None, #Url address for the search result\n",
    "                     results:str ='leads' #Kind of record to scrap, it can be 'leads', 'accounts', 'skills', 'data'. Default 'leads'\n",
    "                    )->pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Method that load the url and scrap the records present on it. It returns only the kind of record selected\n",
    "        \"\"\"\n",
    "        reload = True\n",
    "\n",
    "        #IF reload True (default is True)\n",
    "        while reload :\n",
    "            # self.scroll_bottom(t_time= random.randint(5,7), move=False)\n",
    "            \n",
    "            leads, accounts, skills, data = parse_HAR(har = self.proxy.har)\n",
    "            if leads.shape[0] == 0 or leads.loc[leads.lastName.isnull()].shape[0] > 0 or leads.loc[leads.lastName == ''].shape[0] > 0:\n",
    "                print(f\"Reloading with number of leads {leads.shape[0]}\")\n",
    "                self.driver.refresh()\n",
    "                time.sleep(5)\n",
    "            else:\n",
    "                reload = False\n",
    "        # print(leads.shape)    \n",
    "        if results == 'leads':\n",
    "            return leads\n",
    "        elif results == 'accounts':\n",
    "            return accounts\n",
    "        elif results == 'skills':\n",
    "            return skills\n",
    "        elif results == 'data':\n",
    "            return data\n",
    "        else:\n",
    "            return\n",
    "\n",
    "    def get_data(self,\n",
    "                 url:str = None,\n",
    "                 results:str = 'leads',\n",
    "                 scrol_time:int=60,\n",
    "                 scrol:int = True\n",
    "                ):\n",
    "        self.proxy.new_har(f\"list.har\", options={'captureHeaders': False,'captureContent': True, 'captureBinaryContent': True})\n",
    "        if url:\n",
    "            self.driver.get(url)\n",
    "        else:\n",
    "            self.driver.refresh()\n",
    "        time.sleep(random.randint(4,5))\n",
    "        soup = BeautifulSoup(self.driver.page_source, 'html.parser')\n",
    "        if soup.find_all(text='Too Many Requests') or soup.find_all(text='Sorry something went wrong'):\n",
    "            print(f\"Error to many Requests\")\n",
    "            self.close()\n",
    "            raise RequestException()\n",
    "        if soup.find_all(text='Page Not Found'):\n",
    "            return None\n",
    "        self.scroll_bottom(t_time=scrol_time) if scrol else time.sleep(scrol_time)\n",
    "        leads, accounts, skills, data = parse_HAR(har = self.proxy.har)\n",
    "        if results == 'leads':\n",
    "            leads = leads.merge(skills, on='entityUrn', how='left') if skills.shape[0]>0 else leads\n",
    "            if leads.shape[0] == 0:\n",
    "                return None\n",
    "            return leads\n",
    "        elif results == 'accounts':\n",
    "            return accounts\n",
    "        elif results == 'skills':\n",
    "            return skills\n",
    "        elif results == 'data':\n",
    "            return data\n",
    "        else:\n",
    "            return leads, accounts, skills, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; \n",
    "nbdev.nbdev_export()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sales_Navigator",
   "language": "python",
   "name": "sales_navigator"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
